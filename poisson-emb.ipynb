{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is an implementation for exponential family embeddings\n",
    "# here we start from Poisson embedding\n",
    "'''\n",
    "Author: Disi Ji\n",
    "'''\n",
    "import numpy as np\n",
    "from math import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def PoissonEmb(words,contexts,n_w,N,K=2):\n",
    "    \n",
    "    '''\n",
    "    INPUT:\n",
    "        words: a flat list of [[word in article] article in corpus]\n",
    "        context: [(c: n_c) for c[word] in words], a list of dictionaries\n",
    "        n_w: [count of w for w in words], of same shape as words\n",
    "        N: number of words that embeddings and context embeddings should be learned\n",
    "        K: dimention of embeddings\n",
    "    OUTPUT:\n",
    "        rho: N * K matrix of item embeddings\n",
    "        alpha: N * K matrix of context embeddings, normalized by row\n",
    "    '''\n",
    "    \n",
    "    def contextvector(context,alpha):\n",
    "        # context is a dict\n",
    "        if len(context)==0:\n",
    "            return np.matrix(np.zeros(alpha.shape[1]))\n",
    "        temp = np.array([alpha[i] for i in context.keys()])\n",
    "        cv = np.matrix(list(context.values())) * temp\n",
    "        return cv\n",
    "    \n",
    "    def PlotEmb2d(rho,alpha,data=1):\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(5, 2.2), sharey=True)\n",
    "        colors = [i for i in range(rho.shape[0])]\n",
    "        ax1.scatter(rho[:,0], rho[:,1], s=data, c=colors, alpha=0.5)\n",
    "        ax1.set_title('rho')\n",
    "        ax2.scatter(alpha[:,0], alpha[:,1], s=data, c=colors, alpha=0.5)\n",
    "        ax2.set_title('alpha')\n",
    "        plt.show()\n",
    "\n",
    "    ######hold out 1% data to check convergence#####\n",
    "    L = len(words)\n",
    "    l = int(L/100)\n",
    "    val = np.random.choice(L, l)\n",
    "    words_val = [words[i] for i in val]\n",
    "    contexts_val = [contexts[i] for i in val]\n",
    "    n_w_val = [n_w[i] for i in val]\n",
    "    for i in sorted(val, reverse=True):\n",
    "        del words[i]\n",
    "        del contexts[i]\n",
    "        del n_w[i]\n",
    "        \n",
    "        \n",
    "    ######set parameters#####\n",
    "    L = len(words)\n",
    "    l = int(L/10) # size of subsample\n",
    "    mu, sigma = 0, 0.1\n",
    "    maxiter = 11\n",
    "    lambd = 100\n",
    "    step = 0.01\n",
    "    likelihood = -inf # log likelihood\n",
    "    epsilon = 0.0001\n",
    "    \n",
    "    \n",
    "    ######store idx of the words and contexts######\n",
    "    idx = dict() # idx[i]: idx of (word, context) pairs where word == i\n",
    "    invidx = dict() # invidx[i] = idx of (word, context) pairs where i in context\n",
    "\n",
    "    for i in range(N):\n",
    "        idx[i] = []\n",
    "        invidx[i] = []\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        idx[words[i]].append(i)\n",
    "        for j in contexts[i]:\n",
    "            invidx[j].append(i)\n",
    "        \n",
    "               \n",
    "    ######initialize the rho and alpha matrix######\n",
    "    rho = np.asmatrix(np.random.normal(mu, sigma, (N,K)))\n",
    "    alpha = np.asmatrix(np.random.normal(mu, sigma, (N,K)))\n",
    "    \n",
    "    PlotEmb2d(rho,alpha)\n",
    "\n",
    "    ######update rho and alpha with gradient descent#######\n",
    "\n",
    "    for iter in range(maxiter):\n",
    "        \n",
    "        # check for convergence\n",
    "        likelihood_new = -lambd/2*((np.square(rho)).sum()+(np.square(alpha)).sum())\n",
    "        for i in range(len(val)):\n",
    "            v = contextvector(contexts_val[i],alpha)\n",
    "            ita = (rho[words[i]]*(v.T))[0,0]\n",
    "            rate = exp(ita)\n",
    "            x = n_w_val[i]\n",
    "            likelihood_new += x*ita - rate\n",
    "        print('Log likelihood of iteration %d: %.6f' % (iter, likelihood_new/len(words_val)))\n",
    "        if abs(likelihood_new - likelihood) < epsilon:\n",
    "            break\n",
    "        likelihood = likelihood_new\n",
    "        \n",
    "\n",
    "        samples = np.random.choice(L, l)\n",
    "        \n",
    "        # update rho\n",
    "        for n in range(N):\n",
    "            idx_samples = [val for val in idx[n] if val in samples]\n",
    "            if len(idx_samples)==0:\n",
    "                continue\n",
    "            gradient = -lambd*rho[n]*0.1\n",
    "            for i in idx_samples:\n",
    "                v = contextvector(contexts[i],alpha)\n",
    "                ita = (rho[words[i]]*(v.T))[0,0]\n",
    "                gradient += (n_w[i] - exp(ita))*v\n",
    "            rho[n] += step*gradient/np.linalg.norm(gradient)\n",
    "        \n",
    "            \n",
    "        # update alpha\n",
    "        for n in range(N):\n",
    "            idx_samples = [val for val in invidx[n] if val in samples]\n",
    "            if len(idx_samples)==0:\n",
    "                continue\n",
    "            gradient = -lambd*alpha[n]*0.1\n",
    "            for i in idx_samples:\n",
    "                v = contextvector(contexts[i],alpha)\n",
    "                ita = (rho[words[i]]*(v.T))[0,0]\n",
    "                gradient += (n_w[i] - exp(ita))*rho[words[i]]*contexts[i][n]\n",
    "            alpha[n] += step*gradient/np.linalg.norm(gradient)\n",
    "\n",
    "        PlotEmb2d(rho,alpha)\n",
    "\n",
    "\n",
    "    return rho,alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-08-26 03:49:41,323 INFO --> Loading all data for area tw_oc\n"
     ]
    }
   ],
   "source": [
    "from utils import file_utils as fu\n",
    "from MakeContext import pairwordcontext\n",
    "#from ExpEmb import PoissonEmb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import random\n",
    "random.seed(1)\n",
    "import numpy as np\n",
    "\n",
    "train, val, test = fu.load_data('tw_oc')\n",
    "data = train + val\n",
    "data[data>50] = 50\n",
    "words,contexts,n_w,N = pairwordcontext(data)\n",
    "rho,alpha = PoissonEmb(words,contexts,n_w,N,K = 2)\n",
    "np.savetxt('rho.out', rho, delimiter=',')\n",
    "np.savetxt('alpha.out', alpha, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
